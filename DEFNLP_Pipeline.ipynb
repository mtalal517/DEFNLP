{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFNLP: Hidden Data Citation Extraction Pipeline\n",
    "\n",
    "This notebook implements the complete three-phase DEFNLP methodology for extracting hidden data citations from scientific publications.\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "- **Phase I**: Data Cleaning & Baseline Modeling (string matching)\n",
    "- **Phase II**: SpaCy NER & BERT QA Modeling (advanced NLP)\n",
    "- **Phase III**: Acronym & Abbreviation Extraction\n",
    "- **Final**: Merge all predictions with transfer learning approach\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Configuration](#config)\n",
    "3. [Utility Functions](#utils)\n",
    "4. [Phase I: Baseline Matching](#phase1)\n",
    "5. [Phase II: NER & QA](#phase2)\n",
    "6. [Phase III: Acronyms](#phase3)\n",
    "7. [Pipeline Integration](#pipeline)\n",
    "8. [Run Inference](#inference)\n",
    "9. [Results Analysis](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "Install required dependencies and download models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pandas numpy transformers torch spacy nltk tqdm scikit-learn\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Set, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# NLTK for stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration <a name=\"config\"></a>\n",
    "\n",
    "Define all hyperparameters and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  GPU Available: True\n",
      "  QA Model: salti/bert-base-multilingual-cased-finetuned-squad\n",
      "  SpaCy Model: en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FILE PATHS\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "# Get the directory where this notebook is located\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "\n",
    "# Use absolute paths to avoid FileNotFoundError\n",
    "TRAIN_CSV = os.path.join(NOTEBOOK_DIR, \"train.csv\")\n",
    "TEST_CSV = os.path.join(NOTEBOOK_DIR, \"test.csv\")\n",
    "TRAIN_JSON_DIR = os.path.join(NOTEBOOK_DIR, \"train\")\n",
    "TEST_JSON_DIR = os.path.join(NOTEBOOK_DIR, \"test\")\n",
    "OUTPUT_DIR = os.path.join(NOTEBOOK_DIR, \"output\")\n",
    "BIG_GOV_DATASETS = os.path.join(NOTEBOOK_DIR, \"big_gov_datasets.txt\")  # Optional external datasets\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "QA_MODEL_NAME = \"salti/bert-base-multilingual-cased-finetuned-squad\"\n",
    "SPACY_MODEL = \"en_core_web_sm\"\n",
    "QA_MAX_ANSWER_LENGTH = 64\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE II: NER & QA SETTINGS\n",
    "# ============================================================================\n",
    "DATA_KEYWORDS = [\n",
    "    \"data\", \"datasource\", \"datasources\", \"dataset\", \"datasets\",\n",
    "    \"database\", \"databases\", \"sample\", \"samples\", \"corpus\",\n",
    "    \"repository\", \"repositories\", \"collection\", \"survey\"\n",
    "]\n",
    "\n",
    "NER_ENTITY_TYPES = [\"DATE\", \"ORG\"]\n",
    "\n",
    "QA_QUESTIONS = [\n",
    "    \"Which datasets are used?\",\n",
    "    \"Which data sources are used?\",\n",
    "    \"What datasets were analyzed?\",\n",
    "    \"Which databases are mentioned?\",\n",
    "    \"What data was collected?\"\n",
    "]\n",
    "\n",
    "CHUNK_SIZE = 3  # Sentences per chunk\n",
    "CHUNK_OVERLAP = 1\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE III: ACRONYM SETTINGS\n",
    "# ============================================================================\n",
    "MIN_ACRONYM_LENGTH = 2\n",
    "MAX_ACRONYM_LENGTH = 10\n",
    "\n",
    "# ============================================================================\n",
    "# OUTPUT SETTINGS\n",
    "# ============================================================================\n",
    "PREDICTION_SEPARATOR = \" | \"\n",
    "MIN_CONFIDENCE = 0.0\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  GPU Available: {USE_GPU}\")\n",
    "print(f\"  QA Model: {QA_MODEL_NAME}\")\n",
    "print(f\"  SpaCy Model: {SPACY_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions <a name=\"utils\"></a>\n",
    "\n",
    "Helper functions for text processing and data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_json_publications(json_dir: str, pub_ids: List[str] = None) -> Dict[str, str]:\n",
    "    \"\"\"Load publication JSON files and merge all text content.\"\"\"\n",
    "    pub_texts = {}\n",
    "    \n",
    "    if not os.path.exists(json_dir):\n",
    "        print(f\"Warning: Directory {json_dir} does not exist\")\n",
    "        return pub_texts\n",
    "    \n",
    "    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=f\"Loading {json_dir}\"):\n",
    "        pub_id = json_file.replace('.json', '')\n",
    "        \n",
    "        if pub_ids is not None and pub_id not in pub_ids:\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(json_dir, json_file)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            text_parts = []\n",
    "            for section in data:\n",
    "                if 'text' in section and section['text']:\n",
    "                    text_parts.append(section['text'])\n",
    "            \n",
    "            pub_texts[pub_id] = ' '.join(text_parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    return pub_texts\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text by removing special characters, emojis, and extra spaces.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.\\,\\-\\(\\)]', ' ', text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, stopwords_set: Set[str]) -> str:\n",
    "    \"\"\"Remove stopwords from text.\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in stopwords_set]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def extract_sentences_with_keywords(text: str, keywords: List[str]) -> List[str]:\n",
    "    \"\"\"Extract sentences containing specific keywords.\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    matching_sentences = []\n",
    "    keywords_lower = [k.lower() for k in keywords]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        sentence_lower = sentence.lower()\n",
    "        if any(keyword in sentence_lower for keyword in keywords_lower):\n",
    "            matching_sentences.append(sentence)\n",
    "    \n",
    "    return matching_sentences\n",
    "\n",
    "\n",
    "def chunk_sentences(sentences: List[str], chunk_size: int = 3, overlap: int = 1) -> List[str]:\n",
    "    \"\"\"Chunk sentences into overlapping groups.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(sentences), chunk_size - overlap):\n",
    "        chunk = sentences[i:i + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk))\n",
    "        \n",
    "        if i + chunk_size >= len(sentences):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def format_prediction_string(datasets: Set[str]) -> str:\n",
    "    \"\"\"Format a set of datasets into a prediction string.\"\"\"\n",
    "    if not datasets:\n",
    "        return \"\"\n",
    "    \n",
    "    cleaned = [d.strip() for d in datasets if d and d.strip()]\n",
    "    unique_sorted = sorted(list(set(cleaned)))\n",
    "    \n",
    "    return PREDICTION_SEPARATOR.join(unique_sorted)\n",
    "\n",
    "\n",
    "def merge_prediction_strings(predictions: List[str]) -> str:\n",
    "    \"\"\"Merge multiple prediction strings, remove duplicates, and sort.\"\"\"\n",
    "    all_preds = set()\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if isinstance(pred, str) and pred:\n",
    "            parts = [p.strip() for p in pred.split('|')]\n",
    "            all_preds.update([p for p in parts if p])\n",
    "    \n",
    "    sorted_preds = sorted(list(all_preds))\n",
    "    return PREDICTION_SEPARATOR.join(sorted_preds)\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase I: Data Cleaning & Baseline Matching <a name=\"phase1\"></a>\n",
    "\n",
    "Baseline string matching for initial dataset identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase I class defined\n"
     ]
    }
   ],
   "source": [
    "class PhaseIBaseline:\n",
    "    \"\"\"Phase I: Data cleaning and baseline matching.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.external_datasets = self._load_external_datasets()\n",
    "    \n",
    "    def _load_external_datasets(self) -> Set[str]:\n",
    "        \"\"\"Load external dataset names from file.\"\"\"\n",
    "        datasets = set()\n",
    "        \n",
    "        if not os.path.exists(BIG_GOV_DATASETS):\n",
    "            print(f\"Note: External datasets file {BIG_GOV_DATASETS} not found (optional)\")\n",
    "            return datasets\n",
    "        \n",
    "        try:\n",
    "            with open(BIG_GOV_DATASETS, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    dataset = line.strip().lower()\n",
    "                    if dataset:\n",
    "                        datasets.add(dataset)\n",
    "            print(f\"Loaded {len(datasets)} external datasets\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading external datasets: {e}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def create_internal_labels(self, train_df: pd.DataFrame) -> Set[str]:\n",
    "        \"\"\"Create set of internal dataset labels from training data.\"\"\"\n",
    "        labels = set()\n",
    "        label_columns = ['dataset_title', 'dataset_label', 'cleaned_label']\n",
    "        \n",
    "        for col in label_columns:\n",
    "            if col in train_df.columns:\n",
    "                values = train_df[col].dropna().unique()\n",
    "                for val in values:\n",
    "                    if isinstance(val, str):\n",
    "                        cleaned = clean_text(val)\n",
    "                        if cleaned:\n",
    "                            labels.add(cleaned)\n",
    "        \n",
    "        print(f\"Created {len(labels)} internal dataset labels\")\n",
    "        return labels\n",
    "    \n",
    "    def process(self, df: pd.DataFrame, json_dir: str, train_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"Run complete Phase I pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE I: DATA CLEANING & BASELINE MODELING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load and merge text\n",
    "        pub_ids = df['Id'].unique().tolist()\n",
    "        pub_texts = load_json_publications(json_dir, pub_ids)\n",
    "        df['text'] = df['Id'].map(pub_texts).fillna('')\n",
    "        \n",
    "        # Clean text\n",
    "        print(\"Cleaning text...\")\n",
    "        df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "        df['cleaned_text'] = df['cleaned_text'].apply(\n",
    "            lambda x: remove_stopwords(x, self.stopwords)\n",
    "        )\n",
    "        \n",
    "        # Create internal labels\n",
    "        internal_labels = set()\n",
    "        if train_df is not None:\n",
    "            internal_labels = self.create_internal_labels(train_df)\n",
    "        \n",
    "        # Perform matching\n",
    "        print(\"Performing baseline matching...\")\n",
    "        phase1_predictions = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Phase I\"):\n",
    "            text = row['cleaned_text']\n",
    "            matches = set()\n",
    "            \n",
    "            # Internal matching\n",
    "            for label in internal_labels:\n",
    "                if label in text:\n",
    "                    matches.add(label)\n",
    "            \n",
    "            # External matching\n",
    "            for dataset in self.external_datasets:\n",
    "                if dataset in text:\n",
    "                    matches.add(dataset)\n",
    "            \n",
    "            phase1_predictions.append(format_prediction_string(matches))\n",
    "        \n",
    "        df['phase1_predictions'] = phase1_predictions\n",
    "        print(f\"✓ Phase I complete\")\n",
    "        return df\n",
    "\n",
    "\n",
    "print(\"✓ Phase I class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase II: SpaCy NER & BERT QA <a name=\"phase2\"></a>\n",
    "\n",
    "Advanced NLP using Named Entity Recognition and Question Answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase II class defined\n"
     ]
    }
   ],
   "source": [
    "class PhaseIINER_QA:\n",
    "    \"\"\"Phase II: Named Entity Recognition and Question Answering.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        device = 0 if USE_GPU else -1\n",
    "        \n",
    "        # Load SpaCy model\n",
    "        print(\"Loading SpaCy model...\")\n",
    "        self.nlp = spacy.load(SPACY_MODEL)\n",
    "        \n",
    "        # Load BERT QA model\n",
    "        print(\"Loading BERT QA model...\")\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=QA_MODEL_NAME,\n",
    "            tokenizer=QA_MODEL_NAME,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Phase II models loaded\")\n",
    "    \n",
    "    def extract_ner_entities(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract named entities using SpaCy.\"\"\"\n",
    "        entities = set()\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in NER_ENTITY_TYPES:\n",
    "                entity_text = clean_text(ent.text)\n",
    "                if entity_text:\n",
    "                    entities.add(entity_text)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def qa_extraction(self, chunks: List[str]) -> Set[str]:\n",
    "        \"\"\"Extract dataset mentions using BERT QA model.\"\"\"\n",
    "        answers = set()\n",
    "        \n",
    "        for question in QA_QUESTIONS:\n",
    "            for chunk in chunks:\n",
    "                if not chunk.strip():\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    result = self.qa_pipeline(\n",
    "                        question=question,\n",
    "                        context=chunk,\n",
    "                        max_answer_len=QA_MAX_ANSWER_LENGTH\n",
    "                    )\n",
    "                    \n",
    "                    if result['score'] >= MIN_CONFIDENCE:\n",
    "                        answer = clean_text(result['answer'])\n",
    "                        if answer:\n",
    "                            answers.add(answer)\n",
    "                \n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return answers\n",
    "    \n",
    "    def process_single_text(self, text: str) -> Set[str]:\n",
    "        \"\"\"Process a single text through Phase II pipeline.\"\"\"\n",
    "        all_extractions = set()\n",
    "        \n",
    "        # Extract sentences with keywords\n",
    "        keyword_sentences = extract_sentences_with_keywords(text, DATA_KEYWORDS)\n",
    "        \n",
    "        if not keyword_sentences:\n",
    "            return all_extractions\n",
    "        \n",
    "        # Chunk sentences\n",
    "        chunks = chunk_sentences(keyword_sentences, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        \n",
    "        # NER extraction\n",
    "        ner_entities = self.extract_ner_entities(text)\n",
    "        all_extractions.update(ner_entities)\n",
    "        \n",
    "        # QA extraction\n",
    "        qa_answers = self.qa_extraction(chunks)\n",
    "        all_extractions.update(qa_answers)\n",
    "        \n",
    "        return all_extractions\n",
    "    \n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run complete Phase II pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE II: SPACY NER & BERT QA MODELING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        phase2_predictions = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Phase II\"):\n",
    "            text = row.get('text', '')\n",
    "            extractions = self.process_single_text(text)\n",
    "            phase2_predictions.append(format_prediction_string(extractions))\n",
    "        \n",
    "        df['phase2_predictions'] = phase2_predictions\n",
    "        print(f\"✓ Phase II complete\")\n",
    "        return df\n",
    "\n",
    "\n",
    "print(\"✓ Phase II class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase III: Acronym & Abbreviation Extraction <a name=\"phase3\"></a>\n",
    "\n",
    "Extract acronyms and match with full forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase III class defined\n"
     ]
    }
   ],
   "source": [
    "class PhaseIIIAcronyms:\n",
    "    \"\"\"Phase III: Acronym and abbreviation extraction.\"\"\"\n",
    "    \n",
    "    def extract_acronyms(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract potential acronyms from text.\"\"\"\n",
    "        acronyms = set()\n",
    "        \n",
    "        # Pattern 1: Uppercase words\n",
    "        pattern1 = r'\\b[A-Z]{' + str(MIN_ACRONYM_LENGTH) + ',' + str(MAX_ACRONYM_LENGTH) + r'}\\b'\n",
    "        matches1 = re.findall(pattern1, text)\n",
    "        acronyms.update(matches1)\n",
    "        \n",
    "        # Pattern 2: Acronyms in parentheses\n",
    "        pattern2 = r'\\(([A-Z]{' + str(MIN_ACRONYM_LENGTH) + ',' + str(MAX_ACRONYM_LENGTH) + r'})\\)'\n",
    "        matches2 = re.findall(pattern2, text)\n",
    "        acronyms.update(matches2)\n",
    "        \n",
    "        return acronyms\n",
    "    \n",
    "    def extract_abbreviation_acronym_pairs(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract abbreviation-acronym pairs from text.\"\"\"\n",
    "        pairs = {}\n",
    "        \n",
    "        pattern = r'([A-Z][a-zA-Z\\s\\-]+?)\\s*\\(([A-Z]{' + str(MIN_ACRONYM_LENGTH) + ',' + str(MAX_ACRONYM_LENGTH) + r'})\\)'\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        for full_form, acronym in matches:\n",
    "            full_form = full_form.strip()\n",
    "            acronym = acronym.strip()\n",
    "            pairs[acronym] = full_form\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def process_single_text(self, text: str, previous_predictions: str) -> Set[str]:\n",
    "        \"\"\"Process a single text through Phase III pipeline.\"\"\"\n",
    "        all_extractions = set()\n",
    "        \n",
    "        # Extract acronyms\n",
    "        acronyms = self.extract_acronyms(text)\n",
    "        \n",
    "        # Extract pairs\n",
    "        acronym_pairs = self.extract_abbreviation_acronym_pairs(text)\n",
    "        \n",
    "        # Match acronyms with previous predictions\n",
    "        if previous_predictions:\n",
    "            prev_preds = [p.strip() for p in previous_predictions.split('|')]\n",
    "            \n",
    "            for acronym in acronyms:\n",
    "                acronym_lower = acronym.lower()\n",
    "                for pred in prev_preds:\n",
    "                    if acronym_lower in pred.lower():\n",
    "                        all_extractions.add(acronym_lower)\n",
    "                        all_extractions.add(pred)\n",
    "        \n",
    "        # Create variants from pairs\n",
    "        for acronym, full_form in acronym_pairs.items():\n",
    "            acronym_clean = clean_text(acronym)\n",
    "            full_form_clean = clean_text(full_form)\n",
    "            \n",
    "            all_extractions.add(acronym_clean)\n",
    "            all_extractions.add(full_form_clean)\n",
    "            all_extractions.add(f\"{full_form_clean} {acronym_clean}\")\n",
    "        \n",
    "        return all_extractions\n",
    "    \n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run complete Phase III pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE III: ACRONYM & ABBREVIATION EXTRACTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        phase3_predictions = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Phase III\"):\n",
    "            text = row.get('text', '')\n",
    "            phase2_preds = row.get('phase2_predictions', '')\n",
    "            \n",
    "            extractions = self.process_single_text(text, phase2_preds)\n",
    "            phase3_predictions.append(format_prediction_string(extractions))\n",
    "        \n",
    "        df['phase3_predictions'] = phase3_predictions\n",
    "        print(f\"✓ Phase III complete\")\n",
    "        return df\n",
    "\n",
    "\n",
    "print(\"✓ Phase III class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline Integration <a name=\"pipeline\"></a>\n",
    "\n",
    "Orchestrate all phases and merge predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline class defined\n"
     ]
    }
   ],
   "source": [
    "class DEFNLPPipeline:\n",
    "    \"\"\"Main pipeline orchestrator for DEFNLP methodology.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INITIALIZING DEFNLP PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.phase1 = PhaseIBaseline()\n",
    "        self.phase2 = PhaseIINER_QA()\n",
    "        self.phase3 = PhaseIIIAcronyms()\n",
    "        \n",
    "        print(\"\\n✓ Pipeline initialized successfully\\n\")\n",
    "    \n",
    "    def merge_all_predictions(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Merge predictions from all three phases.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MERGING ALL PHASE PREDICTIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        final_predictions = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            all_preds = [\n",
    "                row.get('phase1_predictions', ''),\n",
    "                row.get('phase2_predictions', ''),\n",
    "                row.get('phase3_predictions', '')\n",
    "            ]\n",
    "            \n",
    "            merged = merge_prediction_strings(all_preds)\n",
    "            final_predictions.append(merged)\n",
    "        \n",
    "        df['PredictionString'] = final_predictions\n",
    "        print(f\"✓ Merged predictions for {len(df)} publications\")\n",
    "        return df\n",
    "    \n",
    "    def run_inference(self, test_csv: str = TEST_CSV, test_json_dir: str = TEST_JSON_DIR,\n",
    "                     train_csv: str = TRAIN_CSV) -> pd.DataFrame:\n",
    "        \"\"\"Run inference on test data.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RUNNING DEFNLP INFERENCE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load data\n",
    "        test_df = pd.read_csv(test_csv)\n",
    "        train_df = pd.read_csv(train_csv) if os.path.exists(train_csv) else None\n",
    "        \n",
    "        # Run phases\n",
    "        test_df = self.phase1.process(test_df, test_json_dir, train_df)\n",
    "        test_df = self.phase2.process(test_df)\n",
    "        test_df = self.phase3.process(test_df)\n",
    "        \n",
    "        # Merge predictions\n",
    "        test_df = self.merge_all_predictions(test_df)\n",
    "        \n",
    "        # Prepare output\n",
    "        output_df = test_df[['Id', 'PredictionString']].copy()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ INFERENCE COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return output_df\n",
    "\n",
    "\n",
    "print(\"✓ Pipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference <a name=\"inference\"></a>\n",
    "\n",
    "Execute the complete pipeline on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING DEFNLP PIPELINE\n",
      "============================================================\n",
      "Note: External datasets file /content/big_gov_datasets.txt not found (optional)\n",
      "Loading SpaCy model...\n",
      "Loading BERT QA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase II models loaded\n",
      "\n",
      "✓ Pipeline initialized successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipeline = DEFNLPPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\mtala\\\\Desktop\\\\DEFNLP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2025524018.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Change to the notebook directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnotebook_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'c:\\Users\\mtala\\Desktop\\DEFNLP'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Verify the files exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\mtala\\\\Desktop\\\\DEFNLP'"
     ]
    }
   ],
   "source": [
    "# Fix file paths - run this cell before running inference\n",
    "import os\n",
    "\n",
    "# Change to the notebook directory\n",
    "notebook_dir = r'C:\\Users\\mtala\\Desktop\\DEFNLP'\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# Verify the files exist\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"test.csv exists: {os.path.exists('test.csv')}\")\n",
    "print(f\"train.csv exists: {os.path.exists('train.csv')}\")\n",
    "print(f\"test directory exists: {os.path.exists('test')}\")\n",
    "print(f\"train directory exists: {os.path.exists('train')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "predictions = pipeline.run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_DIR, \"predictions1.csv\")\n",
    "predictions.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis <a name=\"results\"></a>\n",
    "\n",
    "Analyze and visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(\"=\"*80)\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total publications: {len(predictions)}\")\n",
    "print(f\"Publications with predictions: {(predictions['PredictionString'] != '').sum()}\")\n",
    "print(f\"Publications without predictions: {(predictions['PredictionString'] == '').sum()}\")\n",
    "\n",
    "# Average number of datasets per publication\n",
    "avg_datasets = predictions['PredictionString'].apply(\n",
    "    lambda x: len([p for p in x.split('|') if p.strip()]) if x else 0\n",
    ").mean()\n",
    "print(f\"Average datasets per publication: {avg_datasets:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample predictions with details\n",
    "print(\"\\nSample prediction with details:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_idx = 0\n",
    "if len(predictions) > 0:\n",
    "    pub_id = predictions.iloc[sample_idx]['Id']\n",
    "    pred_string = predictions.iloc[sample_idx]['PredictionString']\n",
    "    \n",
    "    print(f\"Publication ID: {pub_id}\")\n",
    "    print(f\"\\nExtracted datasets:\")\n",
    "    if pred_string:\n",
    "        datasets = [p.strip() for p in pred_string.split('|')]\n",
    "        for i, dataset in enumerate(datasets, 1):\n",
    "            print(f\"  {i}. {dataset}\")\n",
    "    else:\n",
    "        print(\"  (No datasets found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully implements the complete DEFNLP pipeline:\n",
    "\n",
    "✅ **Phase I**: Baseline string matching (internal + external datasets)  \n",
    "✅ **Phase II**: Advanced NLP (SpaCy NER + BERT QA)  \n",
    "✅ **Phase III**: Acronym and abbreviation extraction  \n",
    "✅ **Integration**: Transfer learning approach merging all predictions  \n",
    "\n",
    "The final predictions are saved to `output/predictions.csv` in the required format:\n",
    "- `Id`: Publication ID\n",
    "- `PredictionString`: Pipe-separated dataset names (sorted alphabetically)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Review the predictions in `output/predictions.csv`\n",
    "2. Adjust configuration parameters if needed\n",
    "3. Add more QA questions or keywords for better coverage\n",
    "4. Fine-tune the BERT model on your specific dataset (optional)\n",
    "5. Submit predictions to evaluation platform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
