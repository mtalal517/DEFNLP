{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFNLP: Hidden Data Citation Extraction Pipeline\n",
    "\n",
    "This notebook implements the complete three-phase DEFNLP methodology for extracting hidden data citations from scientific publications.\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "- **Phase I**: Data Cleaning & Baseline Modeling (string matching)\n",
    "- **Phase II**: SpaCy NER & BERT QA Modeling (advanced NLP)\n",
    "- **Phase III**: Acronym & Abbreviation Extraction\n",
    "- **Final**: Merge all predictions with transfer learning approach\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Configuration](#config)\n",
    "3. [Utility Functions](#utils)\n",
    "4. [Phase I: Baseline Matching](#phase1)\n",
    "5. [Phase II: NER & QA](#phase2)\n",
    "6. [Phase III: Acronyms](#phase3)\n",
    "7. [Pipeline Integration](#pipeline)\n",
    "8. [Run Inference](#inference)\n",
    "9. [Results Analysis](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation <a name=\"setup\"></a>\n",
    "\n",
    "Install required dependencies and download models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# !pip install pandas numpy transformers torch spacy nltk tqdm scikit-learn\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mtala\\Desktop\\DEFNLP\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Set, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# NLTK for stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration <a name=\"config\"></a>\n",
    "\n",
    "Define all hyperparameters and settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  GPU Available: False\n",
      "  QA Model: salti/bert-base-multilingual-cased-finetuned-squad\n",
      "  SpaCy Model: en_core_web_sm\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FILE PATHS\n",
    "# ============================================================================\n",
    "import os\n",
    "\n",
    "# Get the directory where this notebook is located\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "\n",
    "# Use absolute paths to avoid FileNotFoundError\n",
    "TRAIN_CSV = os.path.join(NOTEBOOK_DIR, \"train.csv\")\n",
    "TEST_CSV = os.path.join(NOTEBOOK_DIR, \"test.csv\")\n",
    "TRAIN_JSON_DIR = os.path.join(NOTEBOOK_DIR, \"train\")\n",
    "TEST_JSON_DIR = os.path.join(NOTEBOOK_DIR, \"test\")\n",
    "OUTPUT_DIR = os.path.join(NOTEBOOK_DIR, \"output\")\n",
    "BIG_GOV_DATASETS = os.path.join(NOTEBOOK_DIR, \"big_gov_datasets.txt\")  # Optional external datasets\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "QA_MODEL_NAME = \"salti/bert-base-multilingual-cased-finetuned-squad\"\n",
    "SPACY_MODEL = \"en_core_web_sm\"\n",
    "QA_MAX_ANSWER_LENGTH = 64\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE II: NER & QA SETTINGS\n",
    "# ============================================================================\n",
    "DATA_KEYWORDS = [\n",
    "    \"data\", \"datasource\", \"datasources\", \"dataset\", \"datasets\",\n",
    "    \"database\", \"databases\", \"sample\", \"samples\", \"corpus\",\n",
    "    \"repository\", \"repositories\", \"collection\", \"survey\"\n",
    "]\n",
    "\n",
    "NER_ENTITY_TYPES = [\"DATE\", \"ORG\"]\n",
    "\n",
    "QA_QUESTIONS = [\n",
    "    \"Which datasets are used?\",\n",
    "    \"Which data sources are used?\",\n",
    "    \"What datasets were analyzed?\",\n",
    "    \"Which databases are mentioned?\",\n",
    "    \"What data was collected?\"\n",
    "]\n",
    "\n",
    "CHUNK_SIZE = 3  # Sentences per chunk\n",
    "CHUNK_OVERLAP = 1\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE III: ACRONYM SETTINGS\n",
    "# ============================================================================\n",
    "MIN_ACRONYM_LENGTH = 2\n",
    "MAX_ACRONYM_LENGTH = 10\n",
    "\n",
    "# ============================================================================\n",
    "# OUTPUT SETTINGS\n",
    "# ============================================================================\n",
    "PREDICTION_SEPARATOR = \" | \"\n",
    "MIN_CONFIDENCE = 0.0\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  GPU Available: {USE_GPU}\")\n",
    "print(f\"  QA Model: {QA_MODEL_NAME}\")\n",
    "print(f\"  SpaCy Model: {SPACY_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions <a name=\"utils\"></a>\n",
    "\n",
    "Helper functions for text processing and data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_json_publications(json_dir: str, pub_ids: List[str] = None) -> Dict[str, str]:\n",
    "    \"\"\"Load publication JSON files and merge all text content.\"\"\"\n",
    "    pub_texts = {}\n",
    "    \n",
    "    if not os.path.exists(json_dir):\n",
    "        print(f\"Warning: Directory {json_dir} does not exist\")\n",
    "        return pub_texts\n",
    "    \n",
    "    json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=f\"Loading {json_dir}\"):\n",
    "        pub_id = json_file.replace('.json', '')\n",
    "        \n",
    "        if pub_ids is not None and pub_id not in pub_ids:\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(json_dir, json_file)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            text_parts = []\n",
    "            for section in data:\n",
    "                if 'text' in section and section['text']:\n",
    "                    text_parts.append(section['text'])\n",
    "            \n",
    "            pub_texts[pub_id] = ' '.join(text_parts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {json_file}: {e}\")\n",
    "    \n",
    "    return pub_texts\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean text by removing special characters, emojis, and extra spaces.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove emojis\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-z0-9\\s\\.\\,\\-\\(\\)]', ' ', text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, stopwords_set: Set[str]) -> str:\n",
    "    \"\"\"Remove stopwords from text.\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [w for w in words if w not in stopwords_set]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def extract_sentences_with_keywords(text: str, keywords: List[str]) -> List[str]:\n",
    "    \"\"\"Extract sentences containing specific keywords.\"\"\"\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    matching_sentences = []\n",
    "    keywords_lower = [k.lower() for k in keywords]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        sentence_lower = sentence.lower()\n",
    "        if any(keyword in sentence_lower for keyword in keywords_lower):\n",
    "            matching_sentences.append(sentence)\n",
    "    \n",
    "    return matching_sentences\n",
    "\n",
    "\n",
    "def chunk_sentences(sentences: List[str], chunk_size: int = 3, overlap: int = 1) -> List[str]:\n",
    "    \"\"\"Chunk sentences into overlapping groups.\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(sentences), chunk_size - overlap):\n",
    "        chunk = sentences[i:i + chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(' '.join(chunk))\n",
    "        \n",
    "        if i + chunk_size >= len(sentences):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def format_prediction_string(datasets: Set[str]) -> str:\n",
    "    \"\"\"Format a set of datasets into a prediction string.\"\"\"\n",
    "    if not datasets:\n",
    "        return \"\"\n",
    "    \n",
    "    cleaned = [d.strip() for d in datasets if d and d.strip()]\n",
    "    unique_sorted = sorted(list(set(cleaned)))\n",
    "    \n",
    "    return PREDICTION_SEPARATOR.join(unique_sorted)\n",
    "\n",
    "\n",
    "def merge_prediction_strings(predictions: List[str]) -> str:\n",
    "    \"\"\"Merge multiple prediction strings, remove duplicates, and sort.\"\"\"\n",
    "    all_preds = set()\n",
    "    \n",
    "    for pred in predictions:\n",
    "        if isinstance(pred, str) and pred:\n",
    "            parts = [p.strip() for p in pred.split('|')]\n",
    "            all_preds.update([p for p in parts if p])\n",
    "    \n",
    "    sorted_preds = sorted(list(all_preds))\n",
    "    return PREDICTION_SEPARATOR.join(sorted_preds)\n",
    "\n",
    "\n",
    "print(\"✓ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase I: Data Cleaning & Baseline Matching <a name=\"phase1\"></a>\n",
    "\n",
    "Baseline string matching for initial dataset identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase I class defined\n"
     ]
    }
   ],
   "source": [
    "class PhaseIBaseline:\n",
    "    \"\"\"Phase I: Data cleaning and baseline matching.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.external_datasets = self._load_external_datasets()\n",
    "    \n",
    "    def _load_external_datasets(self) -> Set[str]:\n",
    "        \"\"\"Load external dataset names from file.\"\"\"\n",
    "        datasets = set()\n",
    "        \n",
    "        if not os.path.exists(BIG_GOV_DATASETS):\n",
    "            print(f\"Note: External datasets file {BIG_GOV_DATASETS} not found (optional)\")\n",
    "            return datasets\n",
    "        \n",
    "        try:\n",
    "            with open(BIG_GOV_DATASETS, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    dataset = line.strip().lower()\n",
    "                    if dataset:\n",
    "                        datasets.add(dataset)\n",
    "            print(f\"Loaded {len(datasets)} external datasets\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading external datasets: {e}\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def create_internal_labels(self, train_df: pd.DataFrame) -> Set[str]:\n",
    "        \"\"\"Create set of internal dataset labels from training data.\"\"\"\n",
    "        labels = set()\n",
    "        label_columns = ['dataset_title', 'dataset_label', 'cleaned_label']\n",
    "        \n",
    "        for col in label_columns:\n",
    "            if col in train_df.columns:\n",
    "                values = train_df[col].dropna().unique()\n",
    "                for val in values:\n",
    "                    if isinstance(val, str):\n",
    "                        cleaned = clean_text(val)\n",
    "                        if cleaned:\n",
    "                            labels.add(cleaned)\n",
    "        \n",
    "        print(f\"Created {len(labels)} internal dataset labels\")\n",
    "        return labels\n",
    "    \n",
    "    def process(self, df: pd.DataFrame, json_dir: str, train_df: pd.DataFrame = None) -> pd.DataFrame:\n",
    "        \"\"\"Run complete Phase I pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE I: DATA CLEANING & BASELINE MODELING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load and merge text\n",
    "        pub_ids = df['Id'].unique().tolist()\n",
    "        pub_texts = load_json_publications(json_dir, pub_ids)\n",
    "        df['text'] = df['Id'].map(pub_texts).fillna('')\n",
    "        \n",
    "        # Clean text\n",
    "        print(\"Cleaning text...\")\n",
    "        df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "        df['cleaned_text'] = df['cleaned_text'].apply(\n",
    "            lambda x: remove_stopwords(x, self.stopwords)\n",
    "        )\n",
    "        \n",
    "        # Create internal labels\n",
    "        internal_labels = set()\n",
    "        if train_df is not None:\n",
    "            internal_labels = self.create_internal_labels(train_df)\n",
    "        \n",
    "        # Perform matching\n",
    "        print(\"Performing baseline matching...\")\n",
    "        phase1_predictions = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Phase I\"):\n",
    "            text = row['cleaned_text']\n",
    "            matches = set()\n",
    "            \n",
    "            # Internal matching\n",
    "            for label in internal_labels:\n",
    "                if label in text:\n",
    "                    matches.add(label)\n",
    "            \n",
    "            # External matching\n",
    "            for dataset in self.external_datasets:\n",
    "                if dataset in text:\n",
    "                    matches.add(dataset)\n",
    "            \n",
    "            phase1_predictions.append(format_prediction_string(matches))\n",
    "        \n",
    "        df['phase1_predictions'] = phase1_predictions\n",
    "        print(f\"✓ Phase I complete\")\n",
    "        return df\n",
    "\n",
    "\n",
    "print(\"✓ Phase I class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase II: SpaCy NER & BERT QA <a name=\"phase2\"></a>\n",
    "\n",
    "Advanced NLP using Named Entity Recognition and Question Answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase II class defined\n"
     ]
    }
   ],
   "source": [
    "class PhaseIINER_QA:\n",
    "    \"\"\"Phase II: Named Entity Recognition and Question Answering.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        device = 0 if USE_GPU else -1\n",
    "        \n",
    "        # Load SpaCy model\n",
    "        print(\"Loading SpaCy model...\")\n",
    "        self.nlp = spacy.load(SPACY_MODEL)\n",
    "        \n",
    "        # Load BERT QA model\n",
    "        print(\"Loading BERT QA model...\")\n",
    "        self.qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=QA_MODEL_NAME,\n",
    "            tokenizer=QA_MODEL_NAME,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Phase II models loaded\")\n",
    "    \n",
    "    def extract_ner_entities(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract named entities using SpaCy.\"\"\"\n",
    "        entities = set()\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in NER_ENTITY_TYPES:\n",
    "                entity_text = clean_text(ent.text)\n",
    "                if entity_text:\n",
    "                    entities.add(entity_text)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def qa_extraction(self, chunks: List[str]) -> Set[str]:\n",
    "        \"\"\"Extract dataset mentions using BERT QA model.\"\"\"\n",
    "        answers = set()\n",
    "        \n",
    "        for question in QA_QUESTIONS:\n",
    "            for chunk in chunks:\n",
    "                if not chunk.strip():\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    result = self.qa_pipeline(\n",
    "                        question=question,\n",
    "                        context=chunk,\n",
    "                        max_answer_len=QA_MAX_ANSWER_LENGTH\n",
    "                    )\n",
    "                    \n",
    "                    if result['score'] >= MIN_CONFIDENCE:\n",
    "                        answer = clean_text(result['answer'])\n",
    "                        if answer:\n",
    "                            answers.add(answer)\n",
    "                \n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        return answers\n",
    "    \n",
    "    def process_single_text(self, text: str) -> Set[str]:\n",
    "        \"\"\"Process a single text through Phase II pipeline.\"\"\"\n",
    "        all_extractions = set()\n",
    "        \n",
    "        # Extract sentences with keywords\n",
    "        keyword_sentences = extract_sentences_with_keywords(text, DATA_KEYWORDS)\n",
    "        \n",
    "        if not keyword_sentences:\n",
    "            return all_extractions\n",
    "        \n",
    "        # Chunk sentences\n",
    "        chunks = chunk_sentences(keyword_sentences, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "        \n",
    "        # NER extraction\n",
    "        ner_entities = self.extract_ner_entities(text)\n",
    "        all_extractions.update(ner_entities)\n",
    "        \n",
    "        # QA extraction\n",
    "        qa_answers = self.qa_extraction(chunks)\n",
    "        all_extractions.update(qa_answers)\n",
    "        \n",
    "        return all_extractions\n",
    "    \n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run complete Phase II pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE II: SPACY NER & BERT QA MODELING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        phase2_predictions = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Phase II\"):\n",
    "            text = row.get('text', '')\n",
    "            extractions = self.process_single_text(text)\n",
    "            phase2_predictions.append(format_prediction_string(extractions))\n",
    "        \n",
    "        df['phase2_predictions'] = phase2_predictions\n",
    "        print(f\"✓ Phase II complete\")\n",
    "        return df\n",
    "\n",
    "\n",
    "print(\"✓ Phase II class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Phase III: Acronym & Abbreviation Extraction <a name=\"phase3\"></a>\n",
    "\n",
    "Extract acronyms and match with full forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase III class defined\n"
     ]
    }
   ],
   "source": [
    "class PhaseIIIAcronyms:\n",
    "    \"\"\"Phase III: Acronym and abbreviation extraction.\"\"\"\n",
    "    \n",
    "    def extract_acronyms(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract potential acronyms from text.\"\"\"\n",
    "        acronyms = set()\n",
    "        \n",
    "        # Pattern 1: Uppercase words\n",
    "        pattern1 = r'\\b[A-Z]{' + str(MIN_ACRONYM_LENGTH) + ',' + str(MAX_ACRONYM_LENGTH) + r'}\\b'\n",
    "        matches1 = re.findall(pattern1, text)\n",
    "        acronyms.update(matches1)\n",
    "        \n",
    "        # Pattern 2: Acronyms in parentheses\n",
    "        pattern2 = r'\\(([A-Z]{' + str(MIN_ACRONYM_LENGTH) + ',' + str(MAX_ACRONYM_LENGTH) + r'})\\)'\n",
    "        matches2 = re.findall(pattern2, text)\n",
    "        acronyms.update(matches2)\n",
    "        \n",
    "        return acronyms\n",
    "    \n",
    "    def extract_abbreviation_acronym_pairs(self, text: str) -> Dict[str, str]:\n",
    "        \"\"\"Extract abbreviation-acronym pairs from text.\"\"\"\n",
    "        pairs = {}\n",
    "        \n",
    "        pattern = r'([A-Z][a-zA-Z\\s\\-]+?)\\s*\\(([A-Z]{' + str(MIN_ACRONYM_LENGTH) + ',' + str(MAX_ACRONYM_LENGTH) + r'})\\)'\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        for full_form, acronym in matches:\n",
    "            full_form = full_form.strip()\n",
    "            acronym = acronym.strip()\n",
    "            pairs[acronym] = full_form\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def process_single_text(self, text: str, previous_predictions: str) -> Set[str]:\n",
    "        \"\"\"Process a single text through Phase III pipeline.\"\"\"\n",
    "        all_extractions = set()\n",
    "        \n",
    "        # Extract acronyms\n",
    "        acronyms = self.extract_acronyms(text)\n",
    "        \n",
    "        # Extract pairs\n",
    "        acronym_pairs = self.extract_abbreviation_acronym_pairs(text)\n",
    "        \n",
    "        # Match acronyms with previous predictions\n",
    "        if previous_predictions:\n",
    "            prev_preds = [p.strip() for p in previous_predictions.split('|')]\n",
    "            \n",
    "            for acronym in acronyms:\n",
    "                acronym_lower = acronym.lower()\n",
    "                for pred in prev_preds:\n",
    "                    if acronym_lower in pred.lower():\n",
    "                        all_extractions.add(acronym_lower)\n",
    "                        all_extractions.add(pred)\n",
    "        \n",
    "        # Create variants from pairs\n",
    "        for acronym, full_form in acronym_pairs.items():\n",
    "            acronym_clean = clean_text(acronym)\n",
    "            full_form_clean = clean_text(full_form)\n",
    "            \n",
    "            all_extractions.add(acronym_clean)\n",
    "            all_extractions.add(full_form_clean)\n",
    "            all_extractions.add(f\"{full_form_clean} {acronym_clean}\")\n",
    "        \n",
    "        return all_extractions\n",
    "    \n",
    "    def process(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run complete Phase III pipeline.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE III: ACRONYM & ABBREVIATION EXTRACTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        phase3_predictions = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Phase III\"):\n",
    "            text = row.get('text', '')\n",
    "            phase2_preds = row.get('phase2_predictions', '')\n",
    "            \n",
    "            extractions = self.process_single_text(text, phase2_preds)\n",
    "            phase3_predictions.append(format_prediction_string(extractions))\n",
    "        \n",
    "        df['phase3_predictions'] = phase3_predictions\n",
    "        print(f\"✓ Phase III complete\")\n",
    "        return df\n",
    "\n",
    "\n",
    "print(\"✓ Phase III class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline Integration <a name=\"pipeline\"></a>\n",
    "\n",
    "Orchestrate all phases and merge predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline class defined\n"
     ]
    }
   ],
   "source": [
    "class DEFNLPPipeline:\n",
    "    \"\"\"Main pipeline orchestrator for DEFNLP methodology.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"INITIALIZING DEFNLP PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.phase1 = PhaseIBaseline()\n",
    "        self.phase2 = PhaseIINER_QA()\n",
    "        self.phase3 = PhaseIIIAcronyms()\n",
    "        \n",
    "        print(\"\\n✓ Pipeline initialized successfully\\n\")\n",
    "    \n",
    "    def merge_all_predictions(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Merge predictions from all three phases.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MERGING ALL PHASE PREDICTIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        final_predictions = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            all_preds = [\n",
    "                row.get('phase1_predictions', ''),\n",
    "                row.get('phase2_predictions', ''),\n",
    "                row.get('phase3_predictions', '')\n",
    "            ]\n",
    "            \n",
    "            merged = merge_prediction_strings(all_preds)\n",
    "            final_predictions.append(merged)\n",
    "        \n",
    "        df['PredictionString'] = final_predictions\n",
    "        print(f\"✓ Merged predictions for {len(df)} publications\")\n",
    "        return df\n",
    "    \n",
    "    def run_inference(self, test_csv: str = TEST_CSV, test_json_dir: str = TEST_JSON_DIR,\n",
    "                     train_csv: str = TRAIN_CSV) -> pd.DataFrame:\n",
    "        \"\"\"Run inference on test data.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RUNNING DEFNLP INFERENCE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load data\n",
    "        test_df = pd.read_csv(test_csv)\n",
    "        train_df = pd.read_csv(train_csv) if os.path.exists(train_csv) else None\n",
    "        \n",
    "        # Run phases\n",
    "        test_df = self.phase1.process(test_df, test_json_dir, train_df)\n",
    "        test_df = self.phase2.process(test_df)\n",
    "        test_df = self.phase3.process(test_df)\n",
    "        \n",
    "        # Merge predictions\n",
    "        test_df = self.merge_all_predictions(test_df)\n",
    "        \n",
    "        # Prepare output\n",
    "        output_df = test_df[['Id', 'PredictionString']].copy()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✓ INFERENCE COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return output_df\n",
    "\n",
    "\n",
    "print(\"✓ Pipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference <a name=\"inference\"></a>\n",
    "\n",
    "Execute the complete pipeline on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INITIALIZING DEFNLP PIPELINE\n",
      "============================================================\n",
      "Note: External datasets file c:\\Users\\mtala\\Desktop\\DEFNLP\\big_gov_datasets.txt not found (optional)\n",
      "Loading SpaCy model...\n",
      "Loading BERT QA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase II models loaded\n",
      "\n",
      "✓ Pipeline initialized successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipeline = DEFNLPPipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\mtala\\Desktop\\DEFNLP\n",
      "test.csv exists: True\n",
      "train.csv exists: True\n",
      "test directory exists: True\n",
      "train directory exists: True\n"
     ]
    }
   ],
   "source": [
    "# Fix file paths - run this cell before running inference\n",
    "import os\n",
    "\n",
    "# Change to the notebook directory\n",
    "notebook_dir = r'C:/Users/mtala/Desktop/DEFNLP'\n",
    "os.chdir(notebook_dir)\n",
    "\n",
    "# Verify the files exist\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"test.csv exists: {os.path.exists('test.csv')}\")\n",
    "print(f\"train.csv exists: {os.path.exists('train.csv')}\")\n",
    "print(f\"test directory exists: {os.path.exists('test')}\")\n",
    "print(f\"train directory exists: {os.path.exists('train')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING DEFNLP INFERENCE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PHASE I: DATA CLEANING & BASELINE MODELING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading c:\\Users\\mtala\\Desktop\\DEFNLP\\test: 100%|██████████| 4/4 [00:00<00:00, 179.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text...\n",
      "Created 179 internal dataset labels\n",
      "Performing baseline matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase I: 100%|██████████| 4/4 [00:00<00:00, 185.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase I complete\n",
      "\n",
      "============================================================\n",
      "PHASE II: SPACY NER & BERT QA MODELING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase II: 100%|██████████| 4/4 [04:41<00:00, 70.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase II complete\n",
      "\n",
      "============================================================\n",
      "PHASE III: ACRONYM & ABBREVIATION EXTRACTION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase III: 100%|██████████| 4/4 [00:00<00:00, 107.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Phase III complete\n",
      "\n",
      "============================================================\n",
      "MERGING ALL PHASE PREDICTIONS\n",
      "============================================================\n",
      "✓ Merged predictions for 4 publications\n",
      "\n",
      "============================================================\n",
      "✓ INFERENCE COMPLETE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "predictions = pipeline.run_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Predictions saved to c:\\Users\\mtala\\Desktop\\DEFNLP\\output\\predictions1.csv\n"
     ]
    }
   ],
   "source": [
    "# Save predictions\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_DIR, \"predictions1.csv\")\n",
    "predictions.to_csv(output_path, index=False)\n",
    "print(f\"\\n✓ Predictions saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Analysis <a name=\"results\"></a>\n",
    "\n",
    "Analyze and visualize the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 predictions:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n",
       "      <td>.007 | 1000 | 1000 genomes | 1000 genomes ceu)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n",
       "      <td>(ages 25 to 34 | 0 (preprimary education), 1 (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n",
       "      <td>10, 25 | 16 | 180 | 1800s | 1871 | 1874 | 1937...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n",
       "      <td>12 categories and sub-categories | 18 years or...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  2100032a-7c33-4bff-97ef-690822c43466   \n",
       "1  2f392438-e215-4169-bebf-21ac4ff253e1   \n",
       "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n",
       "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "\n",
       "                                    PredictionString  \n",
       "0  .007 | 1000 | 1000 genomes | 1000 genomes ceu)...  \n",
       "1  (ages 25 to 34 | 0 (preprimary education), 1 (...  \n",
       "2  10, 25 | 16 | 180 | 1800s | 1871 | 1874 | 1937...  \n",
       "3  12 categories and sub-categories | 18 years or...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(\"=\"*80)\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Statistics:\n",
      "================================================================================\n",
      "Total publications: 4\n",
      "Publications with predictions: 4\n",
      "Publications without predictions: 0\n",
      "Average datasets per publication: 302.00\n"
     ]
    }
   ],
   "source": [
    "# Statistics\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total publications: {len(predictions)}\")\n",
    "print(f\"Publications with predictions: {(predictions['PredictionString'] != '').sum()}\")\n",
    "print(f\"Publications without predictions: {(predictions['PredictionString'] == '').sum()}\")\n",
    "\n",
    "# Average number of datasets per publication\n",
    "avg_datasets = predictions['PredictionString'].apply(\n",
    "    lambda x: len([p for p in x.split('|') if p.strip()]) if x else 0\n",
    ").mean()\n",
    "print(f\"Average datasets per publication: {avg_datasets:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample prediction with details:\n",
      "================================================================================\n",
      "Publication ID: 2100032a-7c33-4bff-97ef-690822c43466\n",
      "\n",
      "Extracted datasets:\n",
      "  1. .007\n",
      "  2. 1000\n",
      "  3. 1000 genomes\n",
      "  4. 1000 genomes ceu)\n",
      "  5. 106k participants\n",
      "  6. 15,000 independent subjects with neurocognitive and gwas data for analysis\n",
      "  7. 1936\n",
      "  8. 1993\n",
      "  9. 1997\n",
      "  10. 2004\n",
      "  11. 2007\n",
      "  12. 2008\n",
      "  13. 2009\n",
      "  14. 2010\n",
      "  15. 2011\n",
      "  16. 2012\n",
      "  17. 2013\n",
      "  18. 2014\n",
      "  19. 2014a\n",
      "  20. 2014b\n",
      "  21. 2015\n",
      "  22. 21 cogent datasets\n",
      "  23. 40 years of age or older\n",
      "  24. 905 in 1000 genomes ceu)\n",
      "  25. adni\n",
      "  26. affymetrix\n",
      "  27. age 11\n",
      "  28. age 40\n",
      "  29. all 21 cogent datasets\n",
      "  30. approximately 18 years of age\n",
      "  31. approximately 70 years\n",
      "  32. attempts to pinpoint loci associated with human cognition\n",
      "  33. brainspan\n",
      "  34. cardiovascular health study\n",
      "  35. cardiovascular health study chs\n",
      "  36. carroll\n",
      "  37. caucasian of european ancestry\n",
      "  38. ceu\n",
      "  39. charge\n",
      "  40. chs\n",
      "  41. cogent\n",
      "  42. cogent samples were fully independent from both the ssgac and the charge samples\n",
      "  43. cogent samples were genotyped on commercial illumina or affymetrix genome-wide snp\n",
      "  44. cogent1 gwas\n",
      "  45. cogent1 gwas 905 in 1000 genomes ceu\n",
      "  46. cogent1 gwas 905 in 1000 genomes ceu)\n",
      "  47. cogent2\n",
      "  48. cogent2 ) has resulted in the acquisition of 15,000 independent subjects with neurocognitive and gwas data for analysis\n",
      "  49. cognitive genomics consortium\n",
      "  50. cognitive genomics consortium cogent\n",
      "  51. davies\n",
      "  52. disease neuroimaging initiative\n",
      "  53. disease neuroimaging initiative adni\n",
      "  54. dnase\n",
      "  55. fhs\n",
      "  56. fhs and ncng removed since these studies were part of the cohorts for heart and aging research in genomic epidemiology\n",
      "  57. fhs and ncng removed since these studies were part of the cohorts for heart and aging research in genomic epidemiology charge\n",
      "  58. five datasets\n",
      "  59. framingham heart study\n",
      "  60. framingham heart study, second generation cohort (fhs) sample, the cardiovascular health study (chs) sample, and the norwegian cognitive neurogenetics (ncng) sample\n",
      "  61. g factor data\n",
      "  62. gcta\n",
      "  63. gcta has implemented a mixed-linear-model association\n",
      "  64. gcta has implemented a mixed-linear-model association mlma\n",
      "  65. genetic locus associated with cognitive function and education\n",
      "  66. genetics\n",
      "  67. genetics ping\n",
      "  68. genomes project\n",
      "  69. gwas\n",
      "  70. gwas and neurocognitive data\n",
      "  71. gwas data for analysis\n",
      "  72. hbcs\n",
      "  73. howie et al , and gcta\n",
      "  74. id\n",
      "  75. impute2\n",
      "  76. independent, large-scale collection of cohorts\n",
      "  77. june 2014\n",
      "  78. kdp\n",
      "  79. kernel density plot\n",
      "  80. kernel density plot kdp\n",
      "  81. large-scale\n",
      "  82. large-scale dataset\n",
      "  83. ld\n",
      "  84. lothian birth cohort study\n",
      "  85. mds\n",
      "  86. microarrays\n",
      "  87. mlma\n",
      "  88. ncng\n",
      "  89. neurocognitive\n",
      "  90. neurocognitive and gwas\n",
      "  91. neurocognitive and gwas data for analysis\n",
      "  92. neurocognitive data\n",
      "  93. neurocognitive studies\n",
      "  94. none of the three snps\n",
      "  95. none of the three snps previously associated with education were variants included on commercially available microarrays, and thus were imputed into their datasets , 2013) cogent2 samples that did not have genotypes for the snps of interest\n",
      "  96. norwegian cognitive neurogenetics\n",
      "  97. norwegian cognitive neurogenetics ncng\n",
      "  98. panizzon\n",
      "  99. pediatric imaging, neurocognition, and genetics\n",
      "  100. ping\n",
      "  101. publicly available datasets\n",
      "  102. publicly available datasets were included we kindly thank the investigative teams and staffs of the pediatric imaging, neurocognition, and genetics (ping) study, the alzheimer s disease neuroimaging initiative (adni) project\n",
      "  103. pubmed central\n",
      "  104. qc\n",
      "  105. qtl\n",
      "  106. rs1906252 and g\n",
      "  107. rs4851266\n",
      "  108. rs77910749\n",
      "  109. second generation cohort\n",
      "  110. second generation cohort fhs\n",
      "  111. several publicly available datasets\n",
      "  112. snp\n",
      "  113. ssgac\n",
      "  114. stage 1\n",
      "  115. standard gwas quality control\n",
      "  116. standard gwas quality control (qc)\n",
      "  117. standard gwas quality control qc\n",
      "  118. supplementary information\n",
      "  119. tests missing for more than 5 of the sample in an individual study were excluded all cogent samples\n",
      "  120. the cardiovascular health study\n",
      "  121. the childhood intelligence consortium\n",
      "  122. the cognitive genomics consortium\n",
      "  123. the laboratory for neuro imaging\n",
      "  124. the norwegian cognitive neurogenetics\n",
      "  125. the pediatric imaging,\n",
      "  126. the social science genetic association consortium\n",
      "  127. the social science genetic association consortium ssgac\n",
      "  128. the top 69 snps\n",
      "  129. the university of southern california\n",
      "  130. top 69 snps\n",
      "  131. under 40 years of age\n",
      "  132. ward\n",
      "  133. wood et al, 2014\n",
      "  134. years\n"
     ]
    }
   ],
   "source": [
    "# Sample predictions with details\n",
    "print(\"\\nSample prediction with details:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_idx = 0\n",
    "if len(predictions) > 0:\n",
    "    pub_id = predictions.iloc[sample_idx]['Id']\n",
    "    pred_string = predictions.iloc[sample_idx]['PredictionString']\n",
    "    \n",
    "    print(f\"Publication ID: {pub_id}\")\n",
    "    print(f\"\\nExtracted datasets:\")\n",
    "    if pred_string:\n",
    "        datasets = [p.strip() for p in pred_string.split('|')]\n",
    "        for i, dataset in enumerate(datasets, 1):\n",
    "            print(f\"  {i}. {dataset}\")\n",
    "    else:\n",
    "        print(\"  (No datasets found)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully implements the complete DEFNLP pipeline:\n",
    "\n",
    "✅ **Phase I**: Baseline string matching (internal + external datasets)  \n",
    "✅ **Phase II**: Advanced NLP (SpaCy NER + BERT QA)  \n",
    "✅ **Phase III**: Acronym and abbreviation extraction  \n",
    "✅ **Integration**: Transfer learning approach merging all predictions  \n",
    "\n",
    "The final predictions are saved to `output/predictions.csv` in the required format:\n",
    "- `Id`: Publication ID\n",
    "- `PredictionString`: Pipe-separated dataset names (sorted alphabetically)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Review the predictions in `output/predictions.csv`\n",
    "2. Adjust configuration parameters if needed\n",
    "3. Add more QA questions or keywords for better coverage\n",
    "4. Fine-tune the BERT model on your specific dataset (optional)\n",
    "5. Submit predictions to evaluation platform"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
